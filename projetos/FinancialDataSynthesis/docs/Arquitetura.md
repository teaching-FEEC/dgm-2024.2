# Detalhes da Arquitetura ##
==============================
<div align="center">
    <img src="../Arquitetura_Blocos.png" alt="Arquitetura em Blocos" title="Arquitetura em Blocos" />
    <p><em>Figura 1: Arquitetura da Rede Neural em Blocos.</em></p>
</div>


1.**Entrada (Input):**

As entradas do modelo sÃ£o as sÃ©ries temporais de preÃ§os e as sÃ©ries temporais dos features.

A sÃ©rie temporal dos preÃ§os Ã© dada por:

$$ X_{1:N} = [x(1), x(2), ..., x(N)] $$

As sÃ©ries temporais de cada feature sÃ£o representadas como:

$$ F_{1:N} = [f(1), f(2), ..., f(N)] $$

Essas sÃ©ries sÃ£o agrupadas em um mesmo dataframe, constituindo a entrada do modelo, dado por:

$$ D = [X_{1:N},F_{1:N}] $$ 

2.**Sequenciador das SÃ©ries Temporais:**

Para que os dados possam ser processados pelo modelo Transformer, geramos sequÃªncias de tamanho fixo a partir das sÃ©ries temporais. No nosso caso, utilizamos sequÃªncias de tamanho tam_seq= 24, pois o modelo apresentou melhores resultados com este tamanho. 

As sequÃªncias dos preÃ§os sÃ£o descritas como:

$$ SequÃªncias = [{x(1), x(2), ..., x(24)}] , [{x(2), x(3), ..., x(25)}], ..., [{x(N-24), x(N-23), ..., x(N-1)}] $$

O mesmo procedimento Ã© realizado para os features, ao final, juntamos as sequÃªncias de preÃ§o e features.

Cada sequÃªncia possui um target, valor qual devemos predizer. Para o nosso caso, como cada sequÃªncia tem 24 preÃ§os, devemos predizer o 25Âº elemento (25Âº preÃ§o), logo os targets de cada sequÃªncia sÃ£o dados por:

$$ Targets = [x(25)] , [x(26)], ..., [x(N)] $$

Por exemplo, o target da sequÃªncia $[{x(1), x(2), ..., x(24)}]$ Ã© $x(25)$.

3. **Input Layer:**

A camada de entrada da rede neural recebe as sequÃªncias de entrada. Cada sequÃªncia tem dimensÃ£o (tam_seq, n_features), em que:

 - tam_seq = 24  (tamanho das sequÃªncias).
 - n_features = 7 (nÃºmero de features, sendo eles: Moving Average Convergence Divergence (MACD), Relative Strength Index (RSI), Stochastic Oscillator, Commodity Channel Index, Volume, MACD histogram, Money Flow Index)

4. **Embedding Layer:**

A Embedding Layer Ã© uma camada densa responsÃ¡vel por projetar as sequÃªncias de entrada em um espaÃ§o de dimensÃ£o superior. Isso permite que o modelo capture caracterÃ­sticas mais complexas dos dados.

- FunÃ§Ã£o: transformar as sequÃªncias de entrada de dimensÃ£o (tam_seq, n_features) para (tam_seq, model_dim).
  
- Valor Utilizado:
  
  - model_dim = 64 (dimensÃ£o interna usada nas representaÃ§Ãµes do modelo).
  
- OperaÃ§Ã£o:
 
  AplicaÃ§Ã£o de uma camada densa sem funÃ§Ã£o de ativaÃ§Ã£o:
  
  Embeddings=Dense(model_dim)(SequÃªnciasÂ deÂ Entrada)

  Resultado: um tensor de dimensÃ£o (tam_seq, model_dim).

5. **Positional Encoding:**

   Como o Transformer nÃ£o possui mecanismos recorrentes ou convolucionais, Ã© necessÃ¡rio adicionar informaÃ§Ãµes de posiÃ§Ã£o para que o modelo entenda a ordem sequencial dos dados.

- FunÃ§Ã£o: incorporar informaÃ§Ãµes de posiÃ§Ã£o aos embeddings para que a sequÃªncia temporal seja considerada pelo modelo.
 
- OperaÃ§Ã£o:

GeraÃ§Ã£o de uma matriz de codificaÃ§Ã£o posicional usando funÃ§Ãµes seno e cosseno:

PosEnc=FunÃ§Ã£oÂ PositionalÂ Encoding(ğ‘¡ğ‘ğ‘š_ğ‘ ğ‘’ğ‘,ğ‘šğ‘œğ‘‘ğ‘’ğ‘™_ğ‘‘ğ‘–ğ‘š)

AdiÃ§Ã£o das codificaÃ§Ãµes posicionais aos embeddings:

EmbeddingsÂ Posicionais = Embeddings + PosEnc

6. **Blocos Transformers:**

Os blocos Transformer sÃ£o o nÃºcleo do modelo, permitindo que ele aprenda relaÃ§Ãµes complexas dentro das sequÃªncias.

- FunÃ§Ã£o: processar as sequÃªncias posicionais atravÃ©s de mecanismos de atenÃ§Ã£o e redes feed-forward para capturar dependÃªncias temporais.

- Valores Utilizados:
  
  - num_layers = 2: NÃºmero de blocos Transformer empilhados.
  - num_heads = 8: NÃºmero de cabeÃ§as no mecanismo de atenÃ§Ã£o mÃºltipla.
  - ff_dim = 128: DimensÃ£o da rede feed-forward interna.
  - dropout = 0.2: Taxa de dropout aplicada para evitar overfitting.
     
OperaÃ§Ãµes em Cada Bloco Transformer:

**Multi-Head Attention:**

- FunÃ§Ã£o: permite que o modelo preste atenÃ§Ã£o a diferentes posiÃ§Ãµes na sequÃªncia simultaneamente.
 
- OperaÃ§Ã£o:
  
AttentionÂ Output=MultiHeadAttention(num_heads,key_dim=model_dim)(Input,Input)

AplicaÃ§Ã£o de dropout na saÃ­da de atenÃ§Ã£o.


**ConexÃ£o Residual e NormalizaÃ§Ã£o**:

- FunÃ§Ã£o: facilitar o fluxo de gradientes e estabilizar o treinamento.

- OperaÃ§Ã£o: AdiÃ§Ã£o da entrada do bloco Transformer (Input) Ã  saÃ­da do Layer Multi-Head Attention (Attention Output) e NormalizaÃ§Ã£o.

Output1 = LayerNormalization(Input+AttentionÂ Output)

**Feed-Forward Network (FFN):**

- FunÃ§Ã£o: processar as representaÃ§Ãµes aprendidas para capturar padrÃµes nÃ£o lineares.

- OperaÃ§Ãµes:
  
Primeira camada densa com ativaÃ§Ã£o ReLU e regularizaÃ§Ã£o L2:

FFNÂ Output=Dense(ff_dim,activation= â€²relu â€²,kernel_regularizer=L2)(Output1)

Segunda camada densa que retorna Ã  dimensÃ£o model_dim:

FFNÂ Output=Dense(model_dim,kernel_regularizer=L2)(FFNÂ Output)

AplicaÃ§Ã£o de dropout na saÃ­da da FFN.

**ConexÃ£o Residual e NormalizaÃ§Ã£o (2Âº Vez):**

- OperaÃ§Ã£o:

Output2=LayerNormalization(Output1+FFNÂ Output)

**IteraÃ§Ã£o:** O processo Ã© repetido para cada bloco Transformer (num_layers vezes), atualizando o input a cada iteraÃ§Ã£o.

7. **Global Average Pooling:**

ApÃ³s passar pelos blocos Transformer, reduzimos a dimensionalidade para preparar os dados para a camada de saÃ­da (output layer).

- FunÃ§Ã£o: reduzir a dimensÃ£o sequencial calculando a mÃ©dia das representaÃ§Ãµes ao longo do tempo.
  
- OperaÃ§Ã£o:
  
AplicaÃ§Ã£o de mÃ©dia global na dimensÃ£o temporal: 

Pooled_Output=GlobalAveragePooling1D(Output2)

Resultado: um vetor de dimensÃ£o (model_dim). 

8. **Output Layer:**
   A camada de saÃ­da produz a previsÃ£o final do modelo.

- FunÃ§Ã£o: mapear as representaÃ§Ãµes aprendidas para a dimensÃ£o do target desejado.
  
- Valores Utilizados:
    - output_dim = 1: DimensÃ£o da saÃ­da, correspondente Ã  previsÃ£o do retorno.

- OperaÃ§Ã£o:

AplicaÃ§Ã£o de uma camada densa sem funÃ§Ã£o de ativaÃ§Ã£o:

PrediÃ§Ã£o = Dense(ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡_ğ‘‘ğ‘–ğ‘š)(Pooled_Output)

Resultado: um valor escalar que representa a previsÃ£o do retorno no prÃ³ximo perÃ­odo.
