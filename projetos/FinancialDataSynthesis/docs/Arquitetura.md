# Detalhes da Arquitetura ##
==============================
<div align="center">
    <img src="../Arquitetura_Blocos.png" alt="Arquitetura em Blocos" title="Arquitetura em Blocos" />
    <p><em>Figura 1: Arquitetura da Rede Neural em Blocos.</em></p>
</div>


1.**Input:**

A entrada sÃ£o as sÃ©ries temporais do preÃ§o:

$$ X_{1:N} = [x(1), x(2), ..., x(N)] $$

E as sÃ©ries temporais dos features, que por simplicidade, consideramos apenas uma sÃ©rie temporal, dada por:

$$ F_{1:N} = [f(1), f(2), ..., f(N)] $$

Essas sÃ©ries sÃ£o agrupadas em um mesmo dataframe, dado por:

$$ D = [X_{1:N},F_{1:N}] $$ 

2.**Sequenciador das SÃ©ries Temporais:**

Para que os dados possam ser processados pelos blocos Transformers, geramos sequÃªncias de tamanho fixo. No nosso caso, observamos que as sequÃªncias de tamanho 24 (tam_seq = 24) geraram os melhores resultados. Portanto, as sÃ©ries temporais sÃ£o separadas em sequÃªncias. Por exemplo, sequÃªncias da sÃ©rie temporal de preÃ§os sÃ£o geradas como:

$$ SequÃªncias = [{x(1), x(2), ..., x(24)}] , [{x(2), x(3), ..., x(25)}], ..., [{x(N-24), x(N-23), ..., x(N-1)}] $$

Cada sequÃªncia possui um target, valor qual devemos predizer. Para o nosso caso, como cada sequÃªncia tem 24 preÃ§os, devemos predizer o 25Âº elemento (25Âº preÃ§o), logo os targets de cada sequÃªncia sÃ£o dados por:

$$ Targets = [x(25)] , [x(26)], ..., [x(N)] $$

Por exemplo, o target da sequÃªncia $[{x(1), x(2), ..., x(24)}]$ Ã© $x(25)$.

3. **Layer de Input:**

Representa a entrada da rede neural. No nosso exemplo, sÃ£o sequencias com 24 elementos, para cada feature, alÃ©m dos targets.

4. **Embedding Layer:**

A Embedding Layer Ã© uma camada densa responsÃ¡vel por projetar as sequÃªncias de entrada em um espaÃ§o de dimensÃ£o superior. Isso permite que o modelo capture caracterÃ­sticas mais complexas dos dados.

- FunÃ§Ã£o:
  
   Transformar as sequÃªncias de entrada de dimensÃ£o (tam_seq, nÂº de features) para (tam_seq, model_dim).
  
- Valores Utilizados:
  
  - tam_seq = 24  (tamanho das sequÃªncias).
  - model_dim = 64 (dimensÃ£o interna usada nas representaÃ§Ãµes do modelo).
  - nÂº de features = 7 (Moving Average Convergence Divergence (MACD), Relative Strength Index (RSI), Stochastic Oscillator, Commodity Channel Index, Volume, MACD histogram, Money Flow Index)
  
- OperaÃ§Ã£o:
 
  AplicaÃ§Ã£o de uma camada densa sem funÃ§Ã£o de ativaÃ§Ã£o: Embeddings= Dense(ğ‘šğ‘œğ‘‘ğ‘’ğ‘™_ğ‘‘ğ‘–ğ‘š)(SequÃªnciasÂ deÂ Entrada)
  
  Embeddings=Dense(model_dim)(SequÃªnciasÂ deÂ Entrada)

  Resultado: um tensor de dimensÃ£o (tam_seq, model_dim).

5. **Positional Encoding:**

   Como o Transformer nÃ£o possui mecanismos recorrentes ou convolucionais, Ã© necessÃ¡rio adicionar informaÃ§Ãµes de posiÃ§Ã£o para que o modelo entenda a ordem sequencial dos dados.

- FunÃ§Ã£o:

 Incorporar informaÃ§Ãµes de posiÃ§Ã£o aos embeddings para que a sequÃªncia temporal seja considerada pelo modelo.
 
- OperaÃ§Ã£o:

GeraÃ§Ã£o de uma matriz de codificaÃ§Ã£o posicional usando funÃ§Ãµes seno e cosseno:

PosEnc=FunÃ§Ã£oÂ PositionalÂ Encoding(ğ‘¡ğ‘ğ‘š_ğ‘ ğ‘’ğ‘,ğ‘šğ‘œğ‘‘ğ‘’ğ‘™_ğ‘‘ğ‘–ğ‘š)

AdiÃ§Ã£o das codificaÃ§Ãµes posicionais aos embeddings:

EmbeddingsÂ Posicionais =Embeddings + PosEnc

6. **Blocos Transformers:**

Os blocos Transformer sÃ£o o nÃºcleo do modelo, permitindo que ele aprenda relaÃ§Ãµes complexas dentro das sequÃªncias.

- FunÃ§Ã£o:

Processar as sequÃªncias posicionais atravÃ©s de mecanismos de atenÃ§Ã£o e redes feed-forward para capturar dependÃªncias temporais.

- Valores Utilizados:
- 
  - num_layers = 2: NÃºmero de blocos Transformer empilhados.
  - num_heads = 8: NÃºmero de cabeÃ§as no mecanismo de atenÃ§Ã£o mÃºltipla.
  - ff_dim = 128: DimensÃ£o da rede feed-forward interna.
  - dropout = 0.2: Taxa de dropout aplicada para evitar overfitting.
     
OperaÃ§Ãµes em Cada Bloco Transformer:

**Multi-Head Attention:**

- FunÃ§Ã£o:
  
 Permite que o modelo preste atenÃ§Ã£o a diferentes posiÃ§Ãµes na sequÃªncia simultaneamente.
 
- OperaÃ§Ã£o:

AttentionÂ Output = MultiHeadAttention(ğ‘›ğ‘¢ğ‘š_â„ğ‘’ğ‘ğ‘‘ğ‘ ,key_dim=ğ‘šğ‘œğ‘‘ğ‘’ğ‘™_ğ‘‘ğ‘–ğ‘š)(Input,Input)

AttentionÂ Output=MultiHeadAttention(num_heads,key_dim=model_dim)(Input,Input)

AplicaÃ§Ã£o de dropout na saÃ­da de atenÃ§Ã£o.


**ConexÃ£o Residual e NormalizaÃ§Ã£o**:

- FunÃ§Ã£o:

Facilitar o fluxo de gradientes e estabilizar o treinamento.

- OperaÃ§Ã£o: 

Output1 = LayerNormalization(Input+AttentionÂ Output)
Output1=LayerNormalization(Input+AttentionÂ Output)

**Feed-Forward Network (FFN):**

- FunÃ§Ã£o:

Processar as representaÃ§Ãµes aprendidas para capturar padrÃµes nÃ£o lineares.

- OperaÃ§Ãµes:
  
Primeira camada densa com ativaÃ§Ã£o ReLU e regularizaÃ§Ã£o L2:

FFNÂ Output = Dense(ğ‘“ğ‘“_ğ‘‘ğ‘–ğ‘š,activation=â€²ğ‘Ÿğ‘’ğ‘™ğ‘¢â€²,kernel_regularizer=ğ¿2)(Output1)

FFNÂ Output=Dense(ff_dim,activation= â€²relu â€²,kernel_regularizer=L2)(Output1)

Segunda camada densa que retorna Ã  dimensÃ£o model_dim:

FFNÂ Output=Dense(model_dim,kernel_regularizer=L2)(FFNÂ Output)
AplicaÃ§Ã£o de dropout na saÃ­da da FFN.

ConexÃ£o Residual e NormalizaÃ§Ã£o (2Âº Vez):

- OperaÃ§Ã£o:

Output2=LayerNormalization(Output1+FFNÂ Output)

IteraÃ§Ã£o: O processo acima Ã© repetido para cada bloco Transformer (num_layers vezes), atualizando o input a cada iteraÃ§Ã£o.


